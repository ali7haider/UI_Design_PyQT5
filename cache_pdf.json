{
    "D:/Study/Fiverr Projects/Latest New/9 - Daniel/Data/PDFs\\Ali_ComputerScience_Resume_CV.pdf": "ALI HAIDER\nBS Computer Science\nЛ +92 323 7815502\na ali7haider.dev@gmail.com\nf ali7haider\na ali7haider\nî ali7haider.netlify.app\nSUMMARY\nResourceful and inventive programmer proficient in MERN Stack, Python, JavaScript, C, and C++. Equipped with\na solid foundation in Object-Oriented Programming, data structures, and databases, excelling at developing solu-\ntions to real-world problems. Seeking an exciting and challenging job opportunity with a reputable organization\nthat fosters an environment of learning and growth.\nEDUCATION\nUniversity of Engineering and Technology, Lahore\nBS Computer Science\nNov 2021 – Present\n◦CGPA: 3.1/4.0\n◦Coursework: OOP, DSA, DB, OS, Computer Networks, AI/ML, Computer Vision, and Compiler Construction.\nSKILLS SUMMARY\nProgramming Languages: Python, JavaScript, C++, C, C#, SQL\nFrameworks Libraries: MERN Stack, ReactJS, Next.js, Node.js, Express.js, TensorFlow, PyTorch, YOLO, Qt, .NET\nTools Technologies: VS Code, Git, DBeaver, MongoDB, AWS, Docker, OpenCV, Trello, Slack, Agile & Scrum\nSoft Skills: Problem-Solving, Critical Thinking, Teamwork, Leadership, Communication, Adaptability\nWORK EXPERIENCE\nGoSaaS Labs\nSoftware Engineer Intern\nLahore,Pakistan\nJuly 2024 – Sept 2024\n◦Contributed to the GoSaaS Reporting Service, a production-level project while learning GitFlow.\n◦Collaborated with a team of 3, actively participating in daily stand-up meetings.\n◦Gained valuable insights and technical skills through constant mentorship.\nFiverr\nFreelancer\nMarch 2023 – Present\n◦Completed 35+ Dynamic Desktop Applications with PyQt, Winforms, and WPF.\n◦Achieved Level 1 Seller status maintaining a 5/5 client rating through consistent high-quality deliverables.\n◦Successfully completed projects worth over $2000, with positive client reviews.\nPROJECTS\nSmartSketch - AI-Driven Diagramming Tool (Final Year Project)\nSept 2024 – Present\n◦Leading a 3-member team in developing a tool that converts text descriptions into various diagrams.\n◦Implement features that support manual edits and allow further customization through additional text prompts.\nGoSaaS Reporting Service (Internship)\nAug 2024 – Sept 2024\n◦Developed a reporting system that increased data visualization, enabling dynamic report generation.\n◦Technology Used: React, Node.js, Express.js, PostgreSQL, Material-UI, and Bootstrap.\nHelmet and License Plate Detection (Deep Learning)\nMarch 2024 – April 2024\n◦Implemented YOLOv8, achieving a mAP of 95.445% in detecting helmets and license plates.\nOnline Election System (MERN)\nNov 2023 – Jan 2024\n◦Developed an online election system web application with user-friendly and modern frontend.\n◦Technologies used: HTML, CSS, React.js, Node.js, Express.js, MongoDB, and Bootstrap.\nCERTIFICATIONS\n◦Oracle Cloud Infrastructure AI Certified Foundations Associate\n(Oracle)\n◦SQL: Basic, Intermediate, and Advanced\n(HackerRank)\n◦General Coding Logic\n(TripleByte)\n◦Problem Solving: Basic\n(HackerRank)\nPASSION\nAmbition\nAspiring to become a Solution Architect or\nSenior Software Engineer.\nInterests\nWeb development,Python\nand AI.\nOther Interests\nFootball, Touring, and Exploring places.",
    "D:/Study/Fiverr Projects/Latest New/9 - Daniel/Data/PDFs\\Ali_UOL.pdf": "Name\n\nAli Haider Khan\n\n) PAKISTAN National identity card\n\nwet!\n‘Identity Number | Date of Birth\n38302-9855603-9 | 29.10.2002\n\n2 tet eerent © reac heme one canes a mee een pee emt = sein:\n\n14.09.2021 |: 14.09.2031\nReg No: 2021-CS-38\nCNIC #: 3830298556039\nExpiry Date: 31-8-2025",
    "D:/Study/Fiverr Projects/Latest New/9 - Daniel/Data/PDFs\\DatabaseQuizClient (2) (1).pdf": "Question 1: Finding Keys\n—\nSolution: Part (a)\nStep 1: Compute Attribute Closures\n1. Closure of A:\nA+ = {A}\n(Reflexivity)\nA+ = {A, B, C}\nUse: A →BC (Union)\nA+ = {A, B, C, D}\nUse: B →D (Transitivity)\nA+ = {A, B, C, D, E}\nUse: CD →E (Transitivity)\nSince A+ = {A, B, C, D, E}, A is a superkey.\n—\n2. Closure of B:\nB+ = {B}\n(Reflexivity)\nB+ = {B, D}\nUse: B →D (Union)\nSince B+ ̸= {A, B, C, D, E}, B is not a superkey.\n—\n3. Closure of CD:\nCD+ = {C, D}\n(Reflexivity)\nCD+ = {C, D, E}\nUse: CD →E (Union)\nCD+ = {A, C, D, E}\nUse: E →A (Transitivity)\nCD+ = {A, B, C, D, E}\nUse: A →BC (Transitivity)\nSince CD+ = {A, B, C, D, E}, CD is a superkey.\n—\n4. Closure of E:\nE+ = {E}\n(Reflexivity)\nE+ = {A, E}\nUse: E →A (Union)\nE+ = {A, B, C, E}\nUse: A →BC (Transitivity)\nE+ = {A, B, C, D, E}\nUse: B →D (Transitivity)\nSince E+ = {A, B, C, D, E}, E is a superkey.\n—\n1\nStep 2: Check Minimality\nWe check each superkey for minimality:\n• A: Minimal, as no proper subset of A is a superkey.\n• CD: Minimal, as neither C nor D alone is a superkey.\n• E: Minimal, as no proper subset of E is a superkey.\n—\nStep 3: Final Candidate Keys\nThe candidate keys for R(A, B, C, D, E) are:\n{A, CD, E}\n—\nSolution: Part (b)\nGiven:\nF = {A →BC, CD →E, B →D, E →A}\nQuery: AB →C\nSteps to Compute (AB)+:\n(AB)+ = {A, B}\n(Reflexivity)\n(AB)+ = {A, B, C}\nUse: A →BC (Union)\n(AB)+ = {A, B, C, D}\nUse: B →D (Transitivity)\n(AB)+ = {A, B, C, D, E}\nUse: CD →E (Transitivity)\n—\nResult:\nSince C ∈(AB)+, the functional dependency AB →C is entailed by F.\nQuestion 2: Minimal Cover\n—\n2\nSolution\nStep 1: Split RHS into Singletons\n• No changes needed since all RHS are already single attributes.\n—\nStep 2: Check for Redundant FDs\n• ABC →D: Cannot be removed (without, ABC+ = ABCD).\n• CD →A: Cannot be removed (without, CD+ = CDA).\n• CA →B: Cannot be removed (without, CA+ = CAB).\n• AD →C: Cannot be removed (without, AD+ = ADC).\n• CD →B: Cannot be removed (without, CD+ = CDB).\n—\nStep 3: Try Removing Attributes from LHS\n• For ABC →D:\n– Remove C: AB+ still includes D, so C can be removed.\n– Remove B: AC+ still includes D, so B can be removed.\n– Minimal form: AB →D\n• For CD →A:\n– Cannot remove any attributes since the closure of both C+ and D+ does not\ncontain A.\n• For CA →B:\n– Cannot remove any attributes since C+ does not contain B.\n• For AD →C:\n– Cannot remove any attributes since A+ does not contain C.\n• For CD →B:\n– Cannot remove any attributes since C+ and D+ both do not contain B.\n—\nStep 4: Final Minimal Basis\nAfter Step 2 and Step 3, we have the minimal cover F ′:\nF ′ = {AB →D, CD →A, CA →B, AD →C, CD →B}\n—\nFinal Answer:\nThe minimal basis M of F is:\nM = {AB →D, CD →A, CA →B, AD →C, CD →B}\n3\nQuestion 3: Armstrong’s Axioms\nSolution: Part (a)\n1. Step 1: CD →EF (Given) By Decomposition:\nCD →F\n2. Step 2: AB →C (Given) By Augmentation:\nAB →CD\n(Add D to both sides)\n3. Step 3: AB →CD and CD →F By Union:\nAB →CDF\n(Union of AB →CD and CD →F)\n4. Step 4: AB →CDF By Decomposition:\nAB →F\n(Decompose CDF)\nThus, we have shown that AB →F.\nSolution: Part (b)\n1. Step 1: BE →A (Given) By Augmentation:\nBEF →AF\n(Add F to both sides)\n2. Step 2: BEF →AF and BEF →C By Union:\nBEF →ACF\n(Union of BEF →AF and BEF →C)\n3. Step 3: BEF →ACF By Decomposition:\nBEF →A,\nBEF →C,\nBEF →F\n4. Step 4: BEF →C and C →D By Transitivity:\nBEF →D\n5. Step 5: Combine all dependencies:\nBEF →A, B, C, D, E, F\nTherefore, BEF is a key.\nThus, we have shown that BEF is a key.\n4\nQuestion 4: 3NF, BCNF\nSolution:\nPart 1: Functional Dependencies Inferred\nFrom the information provided, we can infer the following functional dependencies (FDs):\n• companyID →companyName, cityName, country, assets\n• deptID →deptName, companyID, cityName, country, deptMgrID\n• cityID →cityName, country\n• (cityName, country) →cityID\n• (companyName, cityName) →companyID\n• (companyID, deptName) →deptID\n• deptMgrID →deptID\nKeys inferred from the functional dependencies:\n• Primary Key for Company: companyID\n• Primary Key for Department: deptID\n• Primary Key for City: cityID\n• Candidate Key for Company: (companyName, cityName)\n• Candidate Key for Department: (companyID, deptName)\n• Candidate Key for City: (cityName, country)\nPart 2: Design Evaluation and Improvement\nStep 1: Checking for 3NF\nA relation is in 3NF if:\n• It is in 2NF.\n• Every non-prime attribute is non-transitively dependent on every candidate key.\nCompany Relation\n- Candidate Key: companyID - companyID →companyName, cityName, country, assets:\nSatisfies 3NF. - (companyName, cityName) →companyID: Violates 3NF (transitive depen-\ndency exists).\nConclusion: Company does not satisfy 3NF.\n5\nDepartment Relation\n- Candidate Key: deptID - deptID →deptName, companyID, cityName, country, deptMgrID:\nSatisfies 3NF. - companyID →deptName: Violates 3NF (transitive dependency exists).\nConclusion: Department does not satisfy 3NF.\nCity Relation\n- Candidate Key: cityID - cityID →cityName, country: Satisfies 3NF. - (country, cityName) →\ncityID: Satisfies 3NF (no transitive dependency).\nConclusion: City satisfies 3NF.\n—\nStep 2: Checking for BCNF\nA relation is in BCNF if:\n• For every functional dependency X →Y , X is a superkey.\nCompany Relation\n- (companyName, cityName) →companyID: Violates BCNF (not a superkey).\nConclusion: Company violates BCNF.\nDepartment Relation\n- companyID →deptName: Violates BCNF (not a superkey).\nConclusion: Department violates BCNF.\nCity Relation\n- Both cityID →cityName, country and (country, cityName) →cityID:\nSatisfy\nBCNF (superkey conditions are met).\nConclusion: City satisfies BCNF.\n—\nStep 3: Decomposition to Achieve BCNF\nCompany Relation Decomposition\n- Decompose into:\nCompanyInfo(companyID, country, assets)\nand\nCompanyMapping(companyID, companyName, cityName)\nDepartment Relation Decomposition\n- Decompose into:\nDepartmentInfo(deptID, deptMgrID)\nand\nDepartmentMapping(deptID, companyID, deptName)\n6\nCity Relation\n- The City relation is already in BCNF. No changes required.\n—\nFinal Schema\nThe final schema after decomposition is:\n• CompanyInfo (companyID, country, assets)\n• CompanyMapping (companyID, companyName, cityName)\n• DepartmentInfo (deptID, deptMgrID)\n• DepartmentMapping (deptID, companyID, deptName)\n• City (cityID, cityName, country)\nThis schema is now in BCNF, ensuring minimal redundancy and no anomalies.\nQuestion 5: Schedules\nSchedules:\n1. S1: r1(X); r2(Z); r1(Z); r3(X); r3(Y ); w1(X); c1; w3(Y ); c3; r2(Y ); w2(Z); w2(Y ); c2\n2. S2: r1(X); r2(Z); r1(Z); r3(X); r3(Y ); w1(X); w3(Y ); r2(Y ); w2(Z); w2(Y ); c1; c2; c3\n3. S3: r1(X); r2(Z); r3(X); r1(Z); r2(Y ); r3(Y ); w1(X); w2(Z); w3(Y ); w2(Y ); c3; c1; c2\nSolution:\n(a) Schedule S1:\nRecoverable: Yes. All transactions in S1 commit after the transactions they depend\non commit. For example, T2 reads Y written by T3 and commits only after T3 commits.\nACA: Yes. All transactions in S1 read values only from committed transactions. For\nexample, T2 reads Y written by T3, and T3 commits before T2 commits.\nStrict: Yes. In S1, no transaction writes to or reads from a data item written by\nanother transaction until that transaction commits. For example, T3 writes Y , and T2\nwrites Y only after T3 commits.\n(b) Schedule S2:\nRecoverable: Yes. All transactions in S2 commit after the transactions they depend\non commit. For example, T2 reads Y written by T3, and T2 commits only after T3 commits.\nACA: No. T2 reads Y written by T3, but T3 does not commit before T2 reads Y ,\nwhich allows cascading aborts.\nStrict: No. In S2, T2 writes Y after T3 writes Y , but T3 does not commit before T2\nwrites Y , violating strictness.\n7\n(c) Schedule S3:\nRecoverable: Yes. All transactions in S3 commit after the transactions they depend\non commit. For example, T2 reads Y written by T3, and T2 commits only after T3 commits.\nACA: No. T2 reads Y written by T3, but T3 does not commit before T2 reads Y ,\nwhich allows cascading aborts.\nStrict: No. In S3, T2 writes Y after T3 writes Y , but T3 does not commit before T2\nwrites Y , violating strictness.\nQuestion 6: Serializability\nSchedules:\n1. r1(X); r3(X); w1(X); r2(X); w3(X)\n2. r3(X); r2(X); w3(X); r1(X); w1(X)\nSolution:\n(a) r1(X); r3(X); w1(X); r2(X); w3(X)\nConflicting Actions:\nConflicting actions occur when:\n• One operation is a write (w), and the other is a read (r) or write (w) on the same\ndata item by different transactions.\nThe conflicting pairs in this schedule are:\n• r3(X) →w1(X) (conflict: T3 reads X before T1 writes X).\n• r2(X) →w3(X) (conflict: T2 reads X before T3 writes X).\nPrecedence Graph for r1(X); r3(X); w1(X); r2(X); w3(X)\nT1\nT2\nT3\nr3(X) →w1(X)\nw1(X) →r2(X)\nr2(X) →w3(X)\nExplanation: The graph forms a cycle (T3 →T1 →T2 →T3). Thus, the schedule is\nnot conflict serializable.\n8\n(b) r3(X); r2(X); w3(X); r1(X); w1(X)\nConflicting Actions:\nThe conflicting pairs in this schedule are:\n• r2(X) →w3(X) (conflict: T2 reads X before T3 writes X).\n• r1(X) →w3(X) (conflict: T1 reads X before T3 writes X).\n• r1(X) →w1(X) (conflict: T1 reads X before T1 writes X).\nPrecedence Graph for r3(X); r2(X); w3(X); r1(X); w1(X)\nT1\nT2\nT3\nr2(X) →w3(X)\nr1(X) →w3(X)\nr1(X) →w1(X)\nExplanation: The graph does not form a cycle. Hence, the schedule is conflict\nserializable. The equivalent serial schedule is T3 →T2 →T1.\nEquivalent Serial Schedule:\nThe precedence graph shows that:\n• T3 executes before T2.\n• T3 executes before T1.\nThe equivalent serial schedule is: T3 →T2 →T1.\nQuestion 8: Locking\nSolution:\n1. Serializability\nA schedule is serializable if it is equivalent to some serial schedule, i.e., the effect of\nexecuting the transactions in the schedule is the same as executing them one by one in\nsome order.\nJustification: This locking protocol ensures that exclusive locks are held until the\ntransaction completes. Since transactions that modify a data object must hold the ex-\nclusive lock until the end, it guarantees that no other transaction can modify or read the\ndata object during the transaction’s execution. This avoids the possibility of conflicting\noperations between transactions, ensuring the schedule’s serializability.\nConclusion: The protocol ensures serializability.\n9\n2. Conflict-serializability\nA schedule is conflict-serializable if the transactions in the schedule can be reordered to\nform a serial schedule, where the order of conflicting operations (write-read or write-write)\nis preserved.\nJustification: Since the protocol requires an exclusive lock for write operations and\nprevents other transactions from accessing the data object being written to, the order\nof conflicting operations is strictly enforced. This is similar to the two-phase locking\nprotocol, which guarantees conflict-serializability by ensuring no conflicting operations\noccur concurrently.\nConclusion: The protocol ensures conflict-serializability.\n3. Recoverability\nA schedule is recoverable if a transaction commits only after all the transactions it depends\non have committed. This prevents situations where a transaction reads a value written\nby an uncommitted transaction, which could lead to inconsistencies.\nJustification: This locking protocol does not directly enforce recoverability. How-\never, by ensuring exclusive locks are held until transaction completion, it reduces the risk\nof reading uncommitted data. The protocol does not specify any mechanism for tracking\ndependencies between transactions, so it does not guarantee recoverability on its own.\nConclusion: The protocol does not ensure recoverability.\n4. Avoids Cascading Aborts\nA schedule avoids cascading aborts if, whenever a transaction reads data, the data it\nreads is from a committed transaction.\nJustification: Since the protocol does not require locks for read operations, it does\nnot guarantee that a transaction will only read data from committed transactions. If a\ntransaction reads a data object that is later aborted, it might cause cascading aborts.\nTherefore, this protocol does not inherently avoid cascading aborts.\nConclusion: The protocol does not avoid cascading aborts.\n5. Avoids Deadlock\nA schedule avoids deadlock if no transaction is left waiting indefinitely for resources that\nother transactions hold.\nJustification: This protocol involves exclusive locks, and a transaction holds an\nexclusive lock on data objects until it completes. If multiple transactions are attempting\nto acquire locks on the same data object, and each is waiting for the other to release\nthe lock, a deadlock can occur. Since the protocol does not include any mechanisms to\nbreak or prevent circular wait conditions (e.g., transaction timeouts or priority schemes),\nit does not guarantee the avoidance of deadlock.\nConclusion: The protocol does not avoid deadlock.\n10",
    "D:/Study/Fiverr Projects/Latest New/9 - Daniel/Data/PDFs\\Football_SportGala_Proposal.pdf": "7-A-Side Futsal Tournament Rules - RivalChamps\nOverview\nThis document provides the rules and guidelines for the 7-a-side futsal tournament to be held during\nthe Sport Gala. Matches will follow a standard futsal format with specific modifications to suit the\ntournament setup. All participants are expected to comply with these rules and uphold the principles of\nfair play and sportsmanship.\nMatch Structure\nGame Duration\n• Each match will consist of two halves, 15 minutes each.\n• A 5-minute break will be given between halves.\n• In case of a tie at the end of regulation time, a penalty shootout will determine the winner.\nTeams and Substitutions\n• Each team consists of 7 players, including the goalkeeper.\n• A maximum of 4 substitutes is allowed per team.\n• Rolling substitutions are permitted; players may re-enter after being substituted.\nMatch Start and Kick-off\n• Teams must be present on the field at least 10 minutes before the scheduled start time.\n• A coin toss will determine which team starts with the ball.\n• The game begins with a kick-off from the center of the field.\nGameplay Rules\nTeam Composition\n• All players on a team must be from the same session to ensure fair representation and cohesion.\nKick-ins\n• Instead of throw-ins, kick-ins will be used to restart play from the sidelines.\nFouls and Free Kicks\n• Fouls will result in a free kick for the opposing team from the spot of the infraction.\n1\nYellow and Red Cards\n• Yellow cards are issued for minor offenses and cautionary fouls.\n• Red cards result in an immediate send-off, and the offending team will play one player down for 2\nminutes or until the opposing team scores.\n• Accumulation of two yellow cards in one match results in a red card and the player’s ejection.\nPenalty Shootout (for Tied Matches)\n• If a match is tied at the end of regular time, a penalty shootout will decide the winner.\n• If the score remains tied after 3 shots, a sudden-death shootout will occur.\nTournament Conduct and Regulations\nRespect for Referees\n• Referees’ decisions are final on all matters.\n• Dissent or disrespectful behavior towards referees may lead to a caution, ejection, or disciplinary\naction.\nLate Arrival Policy\n• Teams arriving more than 10 minutes late will forfeit the match, awarding victory to the opposing\nteam.\n• If both teams are late, the match may be rescheduled at the discretion of the organizing committee.\nEquipment\n• The organizing committee will provide futsal balls.\n• Players must wear appropriate futsal footwear and team jerseys.\nSpectator Code of Conduct\n• Spectators are expected to support teams positively and respectfully.\n• Any inappropriate behavior from spectators may lead to warnings or removal from the venue.\nRegistration and Fees\n• Each team must register with a complete roster of 11 players (7 starting and 4 substitutes).\n• A registration fee of 1500-2000(will be decided) must be paid before the tournament begins.\nFormat\n• Tournament fixtures and the overall competition format (such as group stage, round-robin, or\nknockout) will be determined based on the final number of registered teams.\nConclusion\nThese rules aim to ensure an enjoyable and fair competition for all participants. Any disputes or issues\nnot explicitly covered in these rules will be resolved by the Sport Gala organizing committee.\n2",
    "D:/Study/Fiverr Projects/Latest New/9 - Daniel/Data/PDFs\\FYP1_Report (2).pdf": "SignConnect\nFYP-1\nSession: 2021-2025\nSubmitted to:\nSir Waqas Ali\nSubmitted by:\nSaleem Malik\n2021-CS-32\nMustafa Riaz\n2021-CS-39\nAbdur Rehman\n2021-CS-44\nAbdul Mannan\n2021-CS-219\nDepartment of Computer Science\nUniversity of Engineering and Technology, Lahore\nPakistan\nContents\n1\nIntroduction\n3\n1.1\nObjective\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.2\nScope\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.3\nVision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.4\nOverview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.5\nConstraints\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2\nThe Overall Description\n4\n2.1\nSoftware Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.2\nProduct Functions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n3\nFunctional Requirements\n5\n3.1\nSign Language and Voice Translation . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3.2\nMeeting App Functional Requirements . . . . . . . . . . . . . . . . . . . . . . .\n5\n3.2.1\nUser Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3.2.2\nMeeting Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3.2.3\nChat System\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4\nNon-Functional Requirements\n7\n5\nProject Planning\n8\n5.1\nTimeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n5.2\nResource Allocation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n6\nDiagrams\n10\n6.1\nUse Case Diagram\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n6.2\nClass Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n6.3\nActivity Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n6.4\nSequence Diagram\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n6.5\nState Diagram (State Machine Diagram) . . . . . . . . . . . . . . . . . . . . . .\n14\n6.6\nEntity-Relationship Diagram (ERD)\n. . . . . . . . . . . . . . . . . . . . . . . .\n15\n6.7\nData Flow Diagram (DFD)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n6.8\nComponent Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n6.9\nDeployment Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n6.10 Context Diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n7\nUse Cases\n20\n7.1\nUC1: Sign Language to Text Translation . . . . . . . . . . . . . . . . . . . . . .\n20\n7.2\nUC2: Speech to Sign Language Translation . . . . . . . . . . . . . . . . . . . . .\n20\n7.3\nUC3: Group Video Meeting Creation . . . . . . . . . . . . . . . . . . . . . . . .\n20\n7.4\nUC4: Emergency Medical Communication . . . . . . . . . . . . . . . . . . . . .\n21\n7.5\nUC5: Educational Session Management . . . . . . . . . . . . . . . . . . . . . . .\n21\n7.6\nUC6: Real-time Conference Translation . . . . . . . . . . . . . . . . . . . . . . .\n21\n7.7\nUC7: User Authentication and Profile Setup . . . . . . . . . . . . . . . . . . . .\n22\n7.8\nUC8: Meeting Recording and Transcription\n. . . . . . . . . . . . . . . . . . . .\n22\n7.9\nUC9: Cross-Platform Session Access\n. . . . . . . . . . . . . . . . . . . . . . . .\n22\n7.10 UC10: Public Announcement Translation . . . . . . . . . . . . . . . . . . . . . .\n23\n7.11 UC11: Real-Time Captioning for Live Events\n. . . . . . . . . . . . . . . . . . .\n23\n7.12 UC12: Airport Announcements Translation\n. . . . . . . . . . . . . . . . . . . .\n23\n1\n8\nImplementation Details:\n26\n8.1\nDetection: WLASL Recognition and Translation . . . . . . . . . . . . . . . . . .\n26\n8.1.1\nOverview\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n8.1.2\nKey Features and Components . . . . . . . . . . . . . . . . . . . . . . . .\n26\n8.2\nGeneration: Progressive Transformers for Sign Language Processing . . . . . . .\n27\n8.2.1\nOverview\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n8.2.2\nKey Features and Components . . . . . . . . . . . . . . . . . . . . . . . .\n27\n8.3\nMeeting App (Django + HTML, CSS, JS) . . . . . . . . . . . . . . . . . . . . .\n29\n8.3.1\nOverview\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n8.3.2\nKey Features and Components . . . . . . . . . . . . . . . . . . . . . . . .\n29\n8.3.3\nTechnologies Used\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n2\n1\nIntroduction\n1.1\nObjective\nThe objective of this project is to develop an inclusive communication platform that facilitates\neffective interaction for sign language users. By integrating advanced sign language detection\nand generation components with a meeting application, this platform aims to remove communi-\ncation barriers and foster inclusivity in various settings such as corporate meetings, educational\nenvironments, and personal interactions.\n1.2\nScope\nThe project focuses on addressing the communication challenges faced by sign language users\nby providing the following key features:\n• Real-time Gesture Detection and Translation:\n– The platform will utilize advanced gesture recognition technologies to detect sign\nlanguage gestures in real-time.\n– Detected gestures will be translated into corresponding text, making it comprehen-\nsible for non-sign language users.\n• Sign Language Animation Generation:\n– Textual inputs from users will be transformed into accurate and expressive sign\nlanguage animations.\n– This feature ensures that sign language users can understand textual information\nwithout any third-party assistance.\n• Seamless Meeting Application Integration:\n– A meeting application will be integrated to facilitate uninterrupted communication\nbetween sign language and non-sign language users.\n– Features will include video conferencing, chat functionality, and support for both\ntextual and sign language inputs.\n1.3\nVision\nThe vision of SignConnect is to bridge communication gaps between the deaf and hearing\ncommunities by providing a seamless, real-time, and natural translation system. The project\naims to foster inclusivity and accessibility across various social, educational, and professional\nenvironments by enabling smooth interactions for individuals with hearing impairments.\n1.4\nOverview\nSignConnect will serve as a comprehensive platform where users can register, schedule meet-\nings, and utilize real-time audio and video communication. The application will support various\nfeatures, including user management, meeting room reservations, and instant messaging. Cen-\ntral to the platform’s functionality will be its real-time translation capabilities, which will allow\nusers to engage in conversations regardless of their preferred communication method. By priori-\ntizing user experience, security, and performance, SignConnect aims to create an effective and\nwelcoming environment for all users, enhancing accessibility and inclusion in communication.\n3\n1.5\nConstraints\nThe project operates under the following constraints:\n• Integration Requirements: The detection, generation, and meeting application com-\nponents currently function independently and require significant effort for seamless inte-\ngration.\n• Accuracy and Latency: Ensuring high accuracy in gesture recognition and animation\ngeneration is critical to maintaining user trust and satisfaction. Latency in processing\nmust be minimized to provide a seamless real-time experience.\n2\nThe Overall Description\n2.1\nSoftware Interfaces\n• User Interface (UI): A customizable and user-friendly interface to interact with Sign-\nConnect. The interface will allow users to toggle between sign language to audio and\naudio to sign language conversion.\n• Backend Server: Manages real-time processing of audio and sign language data using\ntransformer networks, pose estimation models, and speech synthesis engines.\n• Avatar Generation System:\nConverts speech input into animated sign language\navatars that perform signing gestures.\n• API Integration: External applications can leverage SignConnect’s functionalities via\nan API that supports embedding avatars, audio conversion, and live translation into video\nstreams or meeting applications.\n• Databases: Stores user preferences, meeting data, sign language datasets, and system\nlogs.\n2.2\nProduct Functions\nThe product will have the following key functions:\n• Real-time audio to sign language translation.\n• Real-time sign language to audio conversion.\n• Speech-to-text and text-to-speech processing.\n• Customizable sign language avatar generation.\n• Support for integrating avatars in live streams, online meetings, and other media.\n• Multi-tasking transformer networks to ensure accurate cross-modal learning for speech\nand sign translation.\n• User profile and accessibility customization options.\n• Cross-platform compatibility via API.\n• Real-time processing for uninterrupted communication.\n4\n3\nFunctional Requirements\n3.1\nSign Language and Voice Translation\n1. Real-time Speech-to-Sign Translation: The system shall convert live spoken lan-\nguage input into corresponding sign language gestures displayed by an avatar in real-time.\n2. Sign-to-Speech Translation: The system shall recognize sign language gestures from\nlive video feeds and convert them into spoken language audio in real-time.\n3. Avatar Generation: The system shall generate customizable avatars that perform sign\nlanguage gestures accurately based on speech inputs.\n4. Pose Estimation for Sign Language: The system shall use pose estimation to detect\nand translate sign language gestures from video frames using keypoint extraction models.\n5. Speech Encoder: The system shall encode spoken language into context-aware embed-\ndings for accurate translation into sign language gestures.\n6. Pose Decoder: The system shall decode embeddings from the speech encoder into sign\nlanguage poses using a transformer network with self-attention and cross-modal attention.\n7. Text-to-Speech (TTS) Conversion: The system shall convert sign language gestures\nback into textual format and generate natural speech output using TTS technology.\n8. Sign Language Video Generation: The system shall create video animations of the\ngenerated sign language poses and display them as an avatar.\n9. Cross-Modal Discriminator: The system shall ensure consistency between the input\nspeech and the generated sign language video through a cross-modal discriminator.\n3.2\nMeeting App Functional Requirements\n3.2.1\nUser Management\n1. User Registration and Login: The system shall allow users to create accounts using\nan email address and password, or log in using existing credentials.\n2. User Profile Management: The system shall allow users to manage and customize\ntheir profiles, including profile pictures, language preferences (spoken and sign language),\nand avatar settings.\n3. Role-Based Access Control: The system shall assign roles (e.g., host, participant) to\nusers and restrict access to certain features based on their role.\n4. Password Recovery: The system shall provide a password recovery option for users\nwho forget their login credentials.\n5. Two-Factor Authentication (2FA): The system shall provide an option for users to\nenable two-factor authentication for enhanced account security.\n6. User Status and Activity: The system shall allow users to set their status (e.g., online,\naway, busy) and display their last activity within the application.\n5\n3.2.2\nMeeting Management\n1. Schedule Meetings: The system shall allow users to schedule meetings in advance,\nspecifying the date, time, and participants.\n2. Instant Meeting Creation: The system shall allow users to create and start instant\nmeetings without prior scheduling.\n3. Invite Participants: The system shall allow meeting organizers to invite participants\nvia email or direct meeting links.\n4. Join Meetings: The system shall allow users to join meetings through a meeting link\nor by logging into their account and selecting the meeting from their dashboard.\n5. Host Controls: The system shall provide meeting hosts with controls such as mut-\ning/unmuting participants, removing participants, and managing screen sharing.\n6. Real-Time Translation Integration: The system shall provide real-time sign language\nand spoken language translation features within the meeting environment.\n7. Meeting Recording: The system shall allow users to record meetings, including sign\nlanguage translations, for future playback.\n8. Breakout Rooms: The system shall support breakout rooms, allowing meeting hosts\nto create smaller group discussions within the main meeting.\n9. Meeting Notifications: The system shall send meeting reminders and notifications to\nparticipants before the scheduled meeting time.\n10. Meeting Summaries: The system shall provide an automated meeting summary (text-\nbased) to participants after the meeting concludes.\n3.2.3\nChat System\n1. Text Chat During Meetings: The system shall provide a text chat feature within\nmeetings, allowing participants to send messages to the group or specific individuals.\n2. Real-Time Translation of Chat: The system shall translate text-based chat messages\ninto sign language and display them as animated avatars for deaf participants.\n3. Emoji and Reactions: The system shall allow users to send emojis or quick reactions\n(e.g., thumbs up, applause) within the chat system.\n4. Private Messaging: The system shall allow users to send private messages to specific\nparticipants during meetings.\n5. File Sharing: The system shall allow users to upload and share files (e.g., documents,\nimages) within the meeting chat.\n6. Chat History: The system shall store chat history for the duration of the meeting,\nallowing users to scroll back to review messages.\n7. Post-Meeting Chat Logs: The system shall provide an option for participants to\ndownload a transcript of the chat after the meeting concludes.\n8. Moderation Tools: The system shall allow meeting hosts to moderate the chat, includ-\ning muting chat for all participants, blocking specific users, and deleting inappropriate\nmessages.\n6\n4\nNon-Functional Requirements\n• Performance: The system shall process audio and video inputs and generate translations\nwithin 500 milliseconds to ensure real-time communication.\n• Scalability: The system shall be scalable to handle multiple concurrent users across\ndifferent use cases, such as live broadcasts, online meetings, and healthcare interactions.\n• Usability: The system interface shall be intuitive and user-friendly, with simple naviga-\ntion options and customization features for avatars and translations.\n• Security: The system shall implement encryption to protect user data, including video\nfeeds, meeting data, and personal preferences.\n• Reliability: The system shall achieve 99.9% uptime to ensure continuous availability,\nespecially during live events and critical healthcare scenarios.\n• Accessibility: The system shall comply with accessibility standards, ensuring it can\nbe used by individuals with various impairments, including hearing, vision, and mobility\ndisabilities.\n• Compatibility: The system shall be compatible with common operating systems (Win-\ndows, macOS, Linux) and mobile platforms (Android, iOS).\n• Cross-Platform Integration: The system shall be integrable with third-party services\nsuch as Zoom, Microsoft Teams, YouTube, and news broadcasting platforms.\n• Data Privacy: The system shall adhere to data privacy regulations (such as GDPR)\nand ensure the confidentiality of user interactions and stored data.\n• Maintainability: The system codebase shall be modular and follow industry best prac-\ntices to facilitate easy maintenance, updates, and bug fixes.\n7\n5\nProject Planning\n5.1\nTimeline\nPhase 1: Initial Research and Setup (Week 1-2)\n- Research on existing sign language recognition systems.\n- Set up Google Colab environment and identify hardware/software requirements.\n- Define the scope and objectives of the individual modules.\nPhase 2: Detection Module Development (Week 3-7)\n- Week 3-4: Research CNN-LSTM models for gesture recognition.\n- Week 5-6: Implement gesture recognition system using CNN-LSTM.\n- Week 7: Testing and debugging of detection module.\nPhase 3: Generation Module Development (Week 8-12)\n- Week 8-9: Research and implement Progressive Transformer for sign language animation\ngeneration.\n- Week 10-11: Develop and fine-tune textual-to-sign language animation system.\n- Week 12: Testing and fine-tuning of the generation module.\nPhase 4: Meeting Application Development (Week 13-14)\n- Week 13: Develop core features of the video conferencing application (messaging and video\ncalls).\n- Week 14: Independent testing of the meeting application with messaging and video confer-\nencing features.\nPhase 5: Final Testing and Optimization (Week 15-16)\n- Week 15: Testing of each individual module (Detection, Generation, and Meeting).\n- Week 16: Performance optimization, debugging, and documentation.\n5.2\nResource Allocation\nHardware Resources:\n• Google Colab (for model training).\n• 8 GB Nvidia Graphics Card (primarily used for model training and fine-tuning).\nSoftware Resources:\n• Programming Languages: Python (for backend development and model training).\n• Libraries/Frameworks:\n– TensorFlow/Keras, PyTorch (for deep learning models and video processing).\n– OpenCV (for video processing).\n– Django (for backend API).\n– HMTL, CSS, JavaScript (for frontend).\n8\n• Development Tools:\n– Google Colab for model training.\n– VS Code for coding.\n• Version Control: Git/GitHub for version management.\n9\n6\nDiagrams\n6.1\nUse Case Diagram\nThe Use Case Diagram for SignConnect demonstrates the interactions between the system’s\nactors—such as Deaf Users, Hearing Users, Meeting Hosts, and Administrators—and the vari-\nous functions they access. These include user registration, real-time sign language translation,\nmeeting management, chat system, and avatar customization. It illustrates how different users\nengage with the core functionalities like creating a meeting, scheduling, sending messages, and\nusing sign-to-speech and speech-to-sign services.\nFigure 1: Use Case Diagram\n10\n6.2\nClass Diagram\nThe Class Diagram for SignConnect provides a blueprint of the system’s static structure. It\nmodels the key entities such as User, Meeting, Avatar, TranslationService, and ChatMessage,\ndetailing their attributes and methods.\nFor example, the User class contains details such\nas username, email, and avatar preferences, while the Meeting class manages participants,\ntime slots, and real-time translation instances.\nThe relationships between classes highlight\nassociations like how a user may host or attend multiple meetings, and meetings contain chat\nand translation services.\nFigure 2: Class Diagram\n11\n6.3\nActivity Diagram\nThe Activity Diagram outlines the flow of processes within the SignConnect system. It de-\npicts activities such as user registration, creating or joining a meeting, and initiating sign lan-\nguage translation. For example, when a user joins a meeting, the system activates video/audio\nfeeds, initializes the translation service (speech-to-sign or sign-to-speech), and allows interac-\ntion through chat and video. The diagram helps trace the sequence of operations from start to\nend, ensuring a smooth user experience.\nFigure 3: Activity Diagram\n12\n6.4\nSequence Diagram\nThe Sequence Diagram focuses on the interaction between different system components over\ntime, particularly during key operations like meeting creation or real-time translation. It shows\nhow a user schedules a meeting, sends invitations, joins the meeting, and how the translation\nsystem interacts with the avatar generation system and chat features. The messages passed\nbetween objects like User, MeetingService, TranslationService, and AvatarService are shown in\na time sequence to illustrate real-time interactions.\nFigure 4: Sequence Diagram\n13\n6.5\nState Diagram (State Machine Diagram)\nThe State Diagram represents the different states of the system, such as when a user transitions\nfrom Idle to In a Meeting, or when a meeting transitions from Scheduled to Ongoing and\neventually to Completed. The system’s components, such as the TranslationService or Meeting,\nfollow a sequence of states triggered by user actions, such as starting the translation or joining\na meeting. This diagram is crucial in understanding how the system reacts to different events,\nsuch as user interaction or system updates.\nFigure 5: State Diagram\n14\n6.6\nEntity-Relationship Diagram (ERD)\nThe Entity-Relationship Diagram (ERD) outlines the structure of the database for SignCon-\nnect. It shows entities such as Users, Meetings, Messages, Translations, and Avatars, and how\nthey relate to each other. For example, a User entity can participate in multiple Meetings, send\nmultiple Messages, and have a customizable Avatar. The Meeting entity is linked to multiple\nTranslations that occur in real-time during the session. The ERD helps define the relationships\nand constraints between the different data points managed by the system.\nFigure 6: Entity-Relationship Diagram\n15\n6.7\nData Flow Diagram (DFD)\nThe Data Flow Diagram depicts how data moves within the system, starting from user inputs\n(e.g., speech or sign language) to processing by the TranslationService and eventually resulting\nin an avatar-based output or spoken language. The DFD traces how data flows between Users,\nMeetings, Avatars, and the backend services that handle translation and communication. For\nexample, when a user speaks, the audio is processed, converted into sign language gestures,\nand then displayed in real time by an animated avatar.\nFigure 7: Data Flow Diagram (DFD)\n16\n6.8\nComponent Diagram\nThe Component Diagram for SignConnect breaks down the system into individual software\ncomponents, such as the TranslationService, MeetingService, UserService, and ChatService.\nThese components interact through APIs and manage core functionalities like real-time trans-\nlation, meeting scheduling, and user management. The diagram shows how each component\nfunctions independently but integrates to form the overall system, allowing for modular updates\nor maintenance.\nFigure 8: Component Diagram\n17\n6.9\nDeployment Diagram\nThe Deployment Diagram shows the physical deployment of SignConnect’s components across\ndifferent servers and client devices. It includes elements like the Client Device (where the user\naccesses the platform), Cloud Servers hosting the backend services, and Translation Engines\nthat process real-time data. The diagram explains how the software is distributed across phys-\nical or cloud infrastructure, detailing where the core translation, meeting, and communication\nfunctionalities are hosted.\nFigure 9: Deployment Diagram\n18\n6.10\nContext Diagram\nThe Context Diagram provides a high-level overview of how SignConnect interacts with exter-\nnal systems and users. It shows external entities such as Users, Third-Party Platforms (e.g.,\nZoom, YouTube), and external databases interacting with the SignConnect System. The dia-\ngram explains how SignConnect fits into a broader environment by showing user interactions,\nintegrations with external meeting tools, and how data is exchanged with external services like\nspeech-to-text providers or cloud storage solutions.\nFigure 10: Context Diagram\n19\n7\nUse Cases\n7.1\nUC1: Sign Language to Text Translation\n• Actor: Sign language user\n• Scenario: Real-time gesture translation\n• Flow:\n1. User activates camera in application\n2. User performs sign language gestures\n3. System captures video input\n4. Process frames through CNN-LSTM pipeline\n5. Display translated text in real-time\n• Post-condition: Accurate text appears within 500ms\n7.2\nUC2: Speech to Sign Language Translation\n• Actor: Non-sign language user\n• Scenario: Converting speech to sign language\n• Flow:\n1. User activates microphone\n2. User speaks into system\n3. Speech processed through encoder\n4. System generates sign language translation\n5. Displays translation through visual output\n• Post-condition: Accurate sign language translation displayed\n7.3\nUC3: Group Video Meeting Creation\n• Actor: Meeting organizer\n• Scenario: Setting up a mixed-participant meeting\n• Flow:\n1. Organizer creates meeting room\n2. Sets meeting parameters\n3. Configures translation requirements\n4. Generates meeting invites\n5. Distributes access credentials\n• Post-condition: Meeting room ready for participants\n20\n7.4\nUC4: Emergency Medical Communication\n• Actor: Healthcare provider\n• Scenario: Urgent medical situation communication\n• Flow:\n1. Activate emergency protocol\n2. Enable priority processing\n3. Perform medical communication\n4. System provides instant translation\n5. Confirm understanding between parties\n• Post-condition: Clear medical information exchanged\n7.5\nUC5: Educational Session Management\n• Actor: Teacher\n• Scenario: Conducting inclusive classroom session\n• Flow:\n1. Initialize virtual classroom\n2. Enable multi-user translation\n3. Start lecture session\n4. Monitor student comprehension\n5. Manage interactive discussions\n• Post-condition: Successful delivery of educational content\n7.6\nUC6: Real-time Conference Translation\n• Actor: Conference presenter\n• Scenario: Large-scale presentation translation\n• Flow:\n1. Initialize conference system\n2. Enable broadcast mode\n3. Begin presentation\n4. System processes multiple outputs\n5. Deliver synchronized translations\n• Post-condition: All attendees receive accessible content\n21\n7.7\nUC7: User Authentication and Profile Setup\n• Actor: New user\n• Scenario: First-time system access\n• Flow:\n1. User accesses registration page\n2. Enters required information\n3. Sets communication preferences\n4. Verifies account\n5. Completes initial setup\n• Post-condition: User account active and configured\n7.8\nUC8: Meeting Recording and Transcription\n• Actor: Meeting participant\n• Scenario: Recording meeting for documentation\n• Flow:\n1. Request meeting recording\n2. System captures all communications\n3. Processes translations\n4. Generates transcriptions\n5. Saves in accessible format\n• Post-condition: Meeting record available with translations\n7.9\nUC9: Cross-Platform Session Access\n• Actor: Mobile user\n• Scenario: Accessing session from different devices\n• Flow:\n1. User initiates mobile login\n2. System verifies credentials\n3. Syncs user preferences\n4. Adapts interface for device\n5. Enables full functionality\n• Post-condition: Seamless mobile access established\n22\n7.10\nUC10: Public Announcement Translation\n• Actor: Public speaker or event organizer\n• Scenario: Translating public announcements into multiple languages in real-time.\n• Flow:\n1. Speaker activates the translation system.\n2. Speaks into the microphone.\n3. System captures the speech input.\n4. Translates it into multiple target languages simultaneously.\n5. Outputs translations as audio or text on various devices or screens.\n• Post-condition: Public announcement delivered in multiple languages to the audience.\n7.11\nUC11: Real-Time Captioning for Live Events\n• Actor: Event attendees with hearing difficulties.\n• Scenario: Providing live captions for speeches or presentations.\n• Flow:\n1. Speaker delivers a live presentation or speech.\n2. System captures audio input in real-time.\n3. Speech is processed using ASR (Automatic Speech Recognition).\n4. System displays live captions on screens or personal devices.\n• Post-condition: Attendees view accurate captions synchronized with the speaker.\n7.12\nUC12: Airport Announcements Translation\n• Actor: Airport staff and international travelers.\n• Scenario: Translating airport announcements into multiple languages.\n• Flow:\n1. Staff inputs announcement details into the system.\n2. System translates the announcement into the target languages.\n3. Delivers audio or text announcements across the terminal or personal devices.\n• Post-condition: Travelers receive the announcements in their preferred languages.\n23\nTest Cases\nTest Case 1: User Sign-Up\n• Test ID: TC1\n• Description: Verify that a new user can successfully sign up for the application.\n• Input: Username: ”john doe”, Email: ”john@example.com”, Password: ”Password123”.\n• Expected Output: User account is created, and a confirmation email is sent.\nTest Case 2: User Sign-In\n• Test ID: TC2\n• Description: Verify that an existing user can log in with valid credentials.\n• Input: Email: ”john@example.com”, Password: ”Password123”.\n• Expected Output: User is logged in successfully and redirected to the dashboard.\nTest Case 3: Invalid Login Attempt\n• Test ID: TC3\n• Description: Ensure that the system prevents login with invalid credentials.\n• Input: Email: ”john@example.com”, Password: ”WrongPassword”.\n• Expected Output: Error message: ”Invalid email or password.”\nTest Case 4: Meeting Creation\n• Test ID: TC4\n• Description: Verify that users can create a meeting and invite participants.\n• Input: Meeting Title: ”Team Sync”, Participants: 5, Time: ”10:00 AM”.\n• Expected Output: Meeting is created, and invitations are sent to participants.\nTest Case 5: Joining a Meeting\n• Test ID: TC5\n• Description: Verify that a user can join a scheduled meeting using the meeting ID.\n• Input: Meeting ID: ”12345ABC”.\n• Expected Output: User joins the meeting successfully, and audio/video is connected.\nTest Case 6: Speech-to-Text Translation Accuracy\n• Test ID: TC6\n• Description: Verify the accuracy of converting speech to text during a meeting.\n• Input: Live speech: ”Welcome to the team meeting.”\n24\n• Expected Output: Text displayed: ”Welcome to the team meeting.” (Accuracy ¿ 95%).\nTest Case 7: Speech-to-Text Translation with Accents\n• Test ID: TC7\n• Description: Ensure the system handles accented speech accurately.\n• Input: British accent: ”The project is progressing well.”\n• Expected Output: Accurate text conversion with minimal errors: ”The project is\nprogressing well.”\nTest Case 8: Text-to-Text Translation Accuracy\n• Test ID: TC8\n• Description: Verify the accuracy of translating text messages between two languages.\n• Input: Text in English: ”Hello, how are you?”\n• Expected Output: Text in Spanish: ”Hola, ¿c´\nomo est´\nas?”\nTest Case 9: Speech-to-Speech Translation Accuracy\n• Test ID: TC9\n• Description: Ensure the accuracy of translating live speech from one language to an-\nother.\n• Input: Speech in English: ”Good morning, everyone.”\n• Expected Output: Translated speech in French: ”Bonjour tout le monde.”\nTest Case 10: Mute/Unmute Feature\n• Test ID: TC10\n• Description: Verify that a participant can mute and unmute their microphone during\na meeting.\n• Input: Click the ”Mute” button, then click the ”Unmute” button.\n• Expected Output: The microphone is muted, and the icon changes to indicate the\nmute state. When unmuted, audio resumes, and the icon reflects the change.\nTest Case 11: Camera On/Off Feature\n• Test ID: TC11\n• Description: Verify that a participant can turn their camera on and off during a meeting.\n• Input: Click the ”Camera On/Off” button.\n• Expected Output: The video feed toggles between visible (camera on) and hidden\n(camera off), and the icon updates to show the current state.\n25\nTest Case 12: Leave Meeting\n• Test ID: TC12\n• Description: Verify that a participant can leave the meeting and that the system handles\ntheir exit gracefully.\n• Input: Click the ”Leave Meeting” button.\n• Expected Output: The participant exits the meeting, the meeting interface closes, and\na confirmation screen or message is displayed. Remaining participants are notified of the\nuser’s departure.\n8\nImplementation Details:\n8.1\nDetection: WLASL Recognition and Translation\n8.1.1\nOverview\nThe WLASL Recognition and Translation system aims to detect and translate American Sign\nLanguage (ASL) gestures from video data.\nThe WLASL dataset, consisting of video clips\nshowing various ASL signs, is used to train a model capable of recognizing and translating\nthese signs into text.\n8.1.2\nKey Features and Components\n• Dataset (WLASL): The WLASL dataset contains approximately 2,000 signs, captured\nfrom multiple angles and people, resulting in 20,000 video clips. Each video clip represents\na different ASL sign. The dataset is preprocessed by extracting frames from the videos,\nresizing, and normalizing them to ensure consistency across the dataset.\n• Preprocessing:\n– Frame Extraction: Video frames are extracted to capture significant moments of\nthe gesture.\n– Resizing: Images are resized to a standard resolution (e.g., 224x224 pixels) for\nuniformity.\n– Normalization: Pixel values are normalized to a standard scale for consistent model\ninput.\n– Data Augmentation: Random rotations, flips, and translations are used to im-\nprove model robustness.\n• Model Architecture:\n– 3D Convolutional Neural Network (CNN): A 3D CNN is used to extract both\nspatial and temporal features from the video frames. The 3D convolutions handle\nthe sequence of frames in each video.\n– LSTM (Long Short-Term Memory): An LSTM network processes temporal\nsequences of frames, capturing long-term dependencies between them.\n– Softmax Layer: The final output is passed through a softmax layer that produces\na probability distribution over possible ASL words.\n26\n• Architecture Diagram:\nFigure 1: Architecture of the WLASL Detection and Translation Model.\n• Training:\n– Loss Function: Cross-entropy loss is used for classification.\n– Regularization: Dropout and other techniques are used to avoid overfitting.\n– Training Process: The dataset is split into training and validation sets, with\nperiodic evaluation of the model’s performance on the validation set.\n• Evaluation: The model’s accuracy is evaluated using precision, recall, and F1-score,\nwhich are standard metrics for classification tasks.\n• Technologies Used:\n– Deep Learning Frameworks: TensorFlow or PyTorch.\n– Model Training: GPUs or TPUs.\n– Dataset: WLASL dataset.\n– Evaluation Metrics: Precision, Recall, F1-score.\n8.2\nGeneration: Progressive Transformers for Sign Language Pro-\ncessing\n8.2.1\nOverview\nThis repository focuses on improving sign language recognition and translation by using a\nprogressive transformer model. Transformers are used to handle long video sequences effectively\nby focusing on relevant parts of the input using self-attention.\n8.2.2\nKey Features and Components\n• Transformer Architecture:\n– Progressive Transformers: This approach processes long sequences in smaller\nchunks, progressively learning from smaller sections of the video. This technique\nreduces computational cost while capturing temporal dependencies.\n27\n– Self-Attention Mechanism: Transformers use self-attention to process the rela-\ntionships between frames, enabling the model to focus on important parts of the\nvideo.\n– Multiple Transformer Layers: The model consists of several layers of transformer\narchitecture, each with self-attention and feedforward layers.\n• Architecture Diagram:\nFigure 2: Architecture of the Progressive Transformers model.\n• Data Augmentation: The dataset is augmented using various transformations such as\nrotations, scaling, and flipping to make the model more robust. Temporal augmentation\n(random cropping) is also applied to improve generalization.\n• Training:\n– Pre-trained Weights: Pre-trained models like BERT or GPT are used to initialize\nthe transformer model, allowing the system to benefit from prior knowledge learned\nfrom large text corpora.\n– Loss Function: Cross-entropy loss is used for classification.\n– Regularization: Dropout and batch normalization are applied to prevent overfit-\nting.\n• Sign Language Translation: The transformer model generates text or speech based\non the recognized sign language gesture. End-to-end generation of ASL to text or speech\nis achieved.\n28\n• Evaluation: The model’s performance is evaluated using BLEU scores for translation\ntasks and accuracy for sign recognition tasks.\n• Technologies Used:\n– Transformers: Hugging Face’s transformers library.\n– PyTorch: For model implementation and training.\n– Data Augmentation: Used to improve robustness.\n– Evaluation Metrics: BLEU score and accuracy.\n8.3\nMeeting App (Django + HTML, CSS, JS)\n8.3.1\nOverview\nThe Meeting App is a web-based platform built with Django for the backend and HTML, CSS,\nand JavaScript for the frontend. It allows users to sign up, sign in, authenticate via OAuth2,\ncreate meetings, and join meetings.\n8.3.2\nKey Features and Components\n• Sign Up: Users can create an account by providing a username, email, and password.\nThe system ensures email uniqueness and validates password strength. Passwords are\nsecurely hashed and stored using Django’s authentication system.\n• Sign In: Users log in by entering their registered credentials. Django’s authentication\nsystem validates the credentials, allowing users to access the meeting dashboard.\n• Authentication (OAuth2): OAuth2 integration allows users to sign in via third-party\nservices like Google, Facebook, or GitHub. OAuth2 enables secure authentication without\nrequiring a password, making the process faster and more secure.\n• Meeting Creation: Users can create meetings by providing details such as meeting title,\ndate, time, and participants. Upon creation, a unique meeting ID or link is generated,\nwhich can be shared with participants.\n• Join Meeting: Participants can join a meeting by entering a meeting ID or clicking a\nshared link. After entering the meeting, the participant is authenticated and the session\nstarts.\n• Frontend (HTML, CSS, JS): The frontend is built using HTML and CSS to create a\nresponsive and user-friendly interface. JavaScript handles dynamic features such as form\nsubmissions, meeting creation, and joining meetings. Libraries like jQuery or Axios are\nused for making asynchronous HTTP requests to interact with the backend.\n8.3.3\nTechnologies Used\n• Django: For backend logic and user management.\n• HTML/CSS/JS: For frontend development and interactivity.\n• OAuth2: For third-party authentication.\n• WebRTC: For real-time communication features (optional).\n29",
    "D:/Study/Fiverr Projects/Latest New/9 - Daniel/Data/PDFs\\Project_Acceptance_Details_Change.pdf": "Changes I made\nFebruary 3, 2025\n1\nIntroduction\nThis report documents the recent enhancements made to improve the functionality, stability, and usability\nof the GUI application. The key improvements include:\n• Adding new status categories for better classification.\n• Implementing dynamic file paths instead of hardcoded paths.\n• To Improve user navigation, making new UI.\n• Improving error handling using try-catch blocks.\n• Generating an executable (.exe) with optimized settings.\n2\nAdding Status Categories\nA new section was added to categorize data more effectively. The statuses include:\n• Accepted\n• Rejected\n• Minor Deficiencies (highlighted in blue)\n• Removed\nTo improve visibility, the ”Minor Deficiencies” category was given a unique color. This is coming in\nall templates and pdf.\n3\nDynamic File Path Handling\nPreviously, file paths were hardcoded, leading to compatibility issues on different systems. The paths\nwere modified dynamically as follows:\n3.1\nOld Hardcoded Path (Problematic)\noriginal = r’O:\\Field Services Division\\Field Support Center\\Project Acceptance\\PA Excel Exterminato\ntarget = r’O:\\Field Services Division\\Field Support Center\\Project Acceptance\\PA Excel Exterminator\\\n3.2\nNew Dynamic Path (Fixed)\nimport os\npath = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n# Define the temporary directory path\ntemp_dir = os.path.join(path, \"Temp\")\n# Define the document paths\noriginal = os.path.join(temp_dir, \"test.docx\")\ntarget = os.path.join(temp_dir, \"test_copy.docx\")\nThis ensures compatibility across different environments and no issue comes in generating exe.\n1\n4\nAdding a Back Button for Better Navigation\nTo prevent users from getting stuck in the UI, a new UI window is added, allowing them to start instead\nof restarting the application.\nImplementation:\n• The UI was restructured for seamless navigation.\n• All steps were integrated into a single UI.\n• The UI was converted to Python using:\npyuic5 -x ui_file.ui -o ui_file.py\n5\nImproved Error Handling (Try-Catch with MessageBox)\nTo prevent unexpected crashes, try-except blocks were added to handle errors gracefully.\n5.1\nExample Implementation\ntry:\nconfig.read(config_path)\nexcept Exception as e:\nQMessageBox.critical(self, \"Error\", f\"Failed to read configuration: {str(e)}\")\nThis prevents crashes and provides clear error messages.\n6\nGenerating Executable (EXE) with PyInstaller\nTo distribute the application as an executable, the following steps were followed:\n6.1\nStep 1: Generate a ‘.spec‘ file\npyinstaller main.py\nThis generates a ‘main.spec‘ file.\n6.2\nStep 2: Modify the ‘.spec‘ file\n• Include necessary data files: Templates, Temp, config, images.\n• Exclude unnecessary libraries: PySide6/PyQt6 to avoid conflicts.\n• Set packaging options: No console, single-file mode.\n6.3\nStep 3: Build the Executable\npyinstaller main.spec\nThis ensures a compact EXE with only required dependencies.\n7\nConclusion\nRobust error handling prevents crashes, and the optimized EXE packaging process allows seamless dis-\ntribution.\n2",
    "D:/Study/Fiverr Projects/Latest New/9 - Daniel/Data/PDFs\\R.26981.pdf": "Ss oN, A GY A ~ © [0\n\n230 VAC : eal J\nP|\nsavbe SPB/BK8045 ——\n\nA 230 VAC\n24 VDC\nA 24 VDC\n\nPGM25-9-17 Kabelwartel M25 (9.0-17.0mm)\nPGM20-6-13 Kabelwartel M20 (6.0-13.0mm)\nPGM16-4,5-10 Kabelwartel M16 (4.5-10.0mm)\n21415/6308 TL-lamp\n\n21474/6308 Deurschakelaar\n\n17515/6308 Verwarmingselement 150W\n17561/6308 Thermostaat\nLPB/TERMINALS-WEIDMUL-ASS Klemmenstrook Weidmuller\nLPB/SSY6102-7 Automaat 2P 2A\nLPB/SSY6204-7 Automaat 2P 4A\n\nLPB/856112 Scharnieren voor APO kast\nLPB/856058 Polyester kastje 300x 185x175\nMB008B Gepantserde geluiddemper 1\"\nM911INOX Schotdoorvoerkopp. RVS G1\n\nM911/4INOX Schotdoorvoerkopp. RVS G1/4\n\nT1000C2800 Snelheidsregelventiel G1/4\n\nT1000C4800 Snelheidsregelventiel G1/2\n\nSXE9573-Z7 1-81-33N 5/2 el.pneum.ventiel veer ISO1 24VDC Cc\nCQM/22152/3/22 Stel eindplaten ISO1\n\nO;]wo!] sr\n\n+HB5S10\nSPB/BK8045\n\n>>\n\nce}\n\nLans 2 Zuur\nC70 A7064\n\n230 VAC\n\n24 VDC\n\nAl:\né\n\n+HB5S 10}\n\n. . . 1s 2 Osmose\n| (40) fe C70 A7217 a\n\n+—\n\nCQM/22152/3/23 Blindplaat\n\nCQM/22152/3/21 Rijgbare basisplaat ISO1 onderkant\nCQM/22354/3/32 Verloopplaat ISO3 naar ISO1\nM/P43316/13 30mm steker Led 12-24V + Kabel 3m\nSXE9575-Z7 1-81-33N 5/2 el.pneum.ventiel veer ISO3 24VDC\nCQM/22354/3/21 Rijgbare basisplaat ISO3 onderkant\nCQM/22354/3/22 Stel eindplaten ISO3\n\n74504-50 Wandbevestiging\n\n“ ce}\n3\nFE\n3\né\n\ng\n\n+8\nal]a\na\n\nDecabon\nn\n\n2 pe\n\nDecabon B10\n\ngl\n3\n\nLabels aanbrengen\n\n74503-51 Koppelstuk voor frames\n\ng\nelle\naya\n&\n\nY64A-4GA-N1N Frame dubbele uitv. G1/2\nY64A-4GA-N2N Frame dubbele uitv. G1/2\nL64M-NNP-EDN Smeertoestel Micro-Fog\n\n18-013-013 Mano @50 rugaansl. R1/8 0-10 bar\nR64G-NNK-RMN Reduceerventiel R. 10 bar\nF64G-NNN-AD3 Filter 40u. auto. metalen res.\nM911/2INOX Schotdoorvoerkopp. RVS G1/2\n602113148EX 3/2 afsluitkraan G1/2\nKS1483.000/1040 Wandbevestiging kast\n$Z2514.000 Rittal Tekeninghouder A4\n$Z2546.000 Driehoeksleutel 8mm Rittal\n$Z2463.000 Rittal kast insert driekantslot 8mm\n\ntaal 2 inbrengen\n\nDecabon\nn\n\n@10\n\n~| fo}\nAle\na\n\n(0) \\ @) === : | Cabinet fixation\n\nc36_|!\n00108}\n\nKS1490.000/1040 Vergrend. ergoform voor KS1468\nKS1480/1040 Polyester kast 1000x800x300\n\n(43\\14)15\\16) | , ' . . Type . |Description =]\n| ; ; Type Plan:\nsoe Lp gg. ee. | : : Voor samenstelling zie:SPB/BK8045\n\nSIN] wolA] alas] N oa\nafoafoafa}f/a]/a]a]nrn\n\n5 : : 16/06/2017 | EERSTE UITGAVE\nSf \" CE : : Index | Datum Beschrijving\n35 [Coo109 re Studie gemaakt door: Naam: Datum:\n[00117] M.Marchant Getekend:| N.Bredoux | 05/10/16 — ©)\nNagezien: | M.Marchant | 06/10/16\n\nTitel:\n\nTe KWA - REGENERATIE 1 EN 2\nFLUIDA - REGENERATIE 2 (OVEN 3) MET TANKPARK 6\nLEIDINGEN V/D INSTALL., INPLANTINGSPLANNEN, SCHEMA'S+LIJSTEN\n\nPERS- EN INSTRUMENTLUCHT\nPNEUMATISCH SCHEMA\nLANS 2\n\nJ. Kennedylaan, 51 Plannummer: 00.\n9042 Gent , R.26981\nFase: 9 line:\nArcelorMittal | Definitief\nADJ\n\nope tc CA\nRe a CS GC» SG SO , WG; © GO [6\npT pT"
}